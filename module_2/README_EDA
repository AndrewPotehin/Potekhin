Проект 2. Разведывательный анализ данных (exploratory data analysis - EDA).

Цели и задачи проекта:
1. Закрепить полученные теоретические знания о EDA и научиться применять их на практике.
2. Попробовать себя в роли Junior Data Scientist, а это значит необходимо: 
   - провести первичный анализ данных с помощью библиотек pandas, matplotlib.pyplot, seaborn, numpy и других;
   - сформулировать предположения и гипотезы для дальнейшего построения модели;
   - проверить качество данных и очистить их, если это необходимо;
   - определиться с параметрами модели.

Суть проекта — отследить влияние условий жизни учащихся в возрасте от 15 до 22 лет на их успеваемость по математике, 
чтобы на ранней стадии выявлять студентов, находящихся в группе риска
Наша задача на данном этапе - провести первичный анализ и подготовку данных для будущей модели, составить отчёт по его результатам. 

Данные, с котороыми необходимо было работать содержались в таблице размером 30 столбцов(признаков) на 395 строк. 
В этом датасете имелись пропуски, выбросы, задвоенная информация. Присутствовали как числовые, так и строковые (категориальные).

В рамках работы над проектом были выполнены следующие действия:
1. Определен целевой признак (было дано в условии задачи)
2. Рассмотрена корреляционная связь остальных признаков с целевым с помощью seaborn.pairplot() и .corr()
3. Рассмотрено количество разных значений для категориальных признаков, их распределение по целевому признаку
4. В числовых признаках данные приведены к необходимому диапозону, устранены выбросы и явные ошибки.
5. Заполнены пропуски на медианное значение для числовх признаков и модой для категориальных (кроме целевого признака)
6. Удалены лишние признаки и строки. 


Что хотелось бы отметить для себя:
1. Понимал, что работа с датафреймом должна продолжиться, но не зная "всей" картины, что будет дальше, как подготовить данные - 
   тяжело было понять, где нужно остановиться и правильно ли ты двигаешься. Здесь сказывается нехватка опыта, и знаний.
2. В работе над проектом понравилось, что задача целостная и по сути ты обрабатываешь различные аномалии в данных.
   С помощью различных инструментов приводишь данные в порядок, очищаешь от "мусора" и лишнего, чтобы они стали удобоваримыми для чего-то большего.
   Т.е. не просто отвлеченные, не связанные друг с другом вопросы по данным, а общий анализ. 
   И, что еще важнее, зачем эти данные там нужны? Каждый признак. И нужны ли? Выявить связи и закономерности.
3. Тяжело работать с визуализацией данных (matplotlib.pyplot и seaborn), но в то же время - это для меня показалось очень информативно, интересно, и красиво.
   Стоит уделить больше времени изучению этого инструмента.
4. Думаю, что важно и нужно изучать больше информации по EDA и уделять время практике.
   Казалось бы, что эта часть работы достаточно быстрая и поверхностная, но это, как бы, фундамент.
   Дальнейшая работа зависит от того, как мы очистим данные, как преобразуем их, какие закономерности увидим на первоначальном этапе.
   По итогу EDA, данных должно быть необходимое и достатчное количество для дальнейшей работы и в удобном, нужном формате.